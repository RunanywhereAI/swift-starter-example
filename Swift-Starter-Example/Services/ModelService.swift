//
//  ModelService.swift
//  Swift-Starter-Example
//
//  RunAnywhere iOS SDK Starter App - Model Management Service
//

import SwiftUI
import Combine
import RunAnywhere
import LlamaCPPRuntime
import ONNXRuntime

/// Service for managing AI models - handles downloading, loading, and state tracking
@MainActor
final class ModelService: ObservableObject {
    // MARK: - Model IDs (must match registered model IDs)
    static let llmModelId = "lfm2-350m-q4_k_m"
    static let sttModelId = "sherpa-onnx-whisper-tiny.en"
    static let ttsModelId = "vits-piper-en_US-lessac-medium"
    static let vlmModelId = "smolvlm-256m-instruct"
    static let diffusionModelId = "sd15-coreml-palettized"
    
    // MARK: - Download State
    @Published var isLLMDownloading = false
    @Published var isSTTDownloading = false
    @Published var isTTSDownloading = false
    @Published var isVLMDownloading = false
    @Published var isDiffusionDownloading = false
    
    @Published var llmDownloadProgress: Double = 0.0
    @Published var sttDownloadProgress: Double = 0.0
    @Published var ttsDownloadProgress: Double = 0.0
    @Published var vlmDownloadProgress: Double = 0.0
    @Published var diffusionDownloadProgress: Double = 0.0
    
    // MARK: - Load State
    @Published var isLLMLoading = false
    @Published var isSTTLoading = false
    @Published var isTTSLoading = false
    @Published var isVLMLoading = false
    @Published var isDiffusionLoading = false
    
    // MARK: - Loaded State
    @Published private(set) var isLLMLoaded = false
    @Published private(set) var isSTTLoaded = false
    @Published private(set) var isTTSLoaded = false
    @Published private(set) var isVLMLoaded = false
    @Published private(set) var isDiffusionLoaded = false
    
    /// Status message for diffusion (loading can take minutes for CoreML compilation)
    @Published var diffusionStatusMessage = ""
    
    // MARK: - Computed Properties
    var isVoiceAgentReady: Bool {
        isLLMLoaded && isSTTLoaded && isTTSLoaded
    }
    
    var isAnyDownloading: Bool {
        isLLMDownloading || isSTTDownloading || isTTSDownloading || isVLMDownloading || isDiffusionDownloading
    }
    
    var isAnyLoading: Bool {
        isLLMLoading || isSTTLoading || isTTSLoading || isVLMLoading || isDiffusionLoading
    }
    
    // MARK: - Initialization
    init() {
        Task {
            await refreshLoadedStates()
        }
    }
    
    // MARK: - Model Registration
    /// Register default models with the SDK
    static func registerDefaultModels() {
        // Register LLM model - LiquidAI LFM2 350M Q4 (small, fast, efficient)
        if let lfm2URL = URL(string: "https://huggingface.co/LiquidAI/LFM2-350M-GGUF/resolve/main/LFM2-350M-Q4_K_M.gguf") {
            RunAnywhere.registerModel(
                id: llmModelId,
                name: "LiquidAI LFM2 350M Q4_K_M",
                url: lfm2URL,
                framework: .llamaCpp,
                memoryRequirement: 250_000_000
            )
        }
        
        // Register STT model - Whisper Tiny (fast, accurate for English)
        if let whisperURL = URL(string: "https://github.com/RunanywhereAI/sherpa-onnx/releases/download/runanywhere-models-v1/sherpa-onnx-whisper-tiny.en.tar.gz") {
            RunAnywhere.registerModel(
                id: sttModelId,
                name: "Sherpa Whisper Tiny (ONNX)",
                url: whisperURL,
                framework: .onnx,
                modality: .speechRecognition,
                artifactType: .archive(.tarGz, structure: .nestedDirectory),
                memoryRequirement: 75_000_000
            )
        }
        
        // Register TTS voice - Piper US English (natural sounding)
        if let piperURL = URL(string: "https://github.com/RunanywhereAI/sherpa-onnx/releases/download/runanywhere-models-v1/vits-piper-en_US-lessac-medium.tar.gz") {
            RunAnywhere.registerModel(
                id: ttsModelId,
                name: "Piper TTS (US English - Medium)",
                url: piperURL,
                framework: .onnx,
                modality: .speechSynthesis,
                artifactType: .archive(.tarGz, structure: .nestedDirectory),
                memoryRequirement: 65_000_000
            )
        }
        
        // Register VLM model - SmolVLM 256M (tiny multimodal model, GGUF + mmproj)
        let vlmModelURL = URL(string: "https://huggingface.co/ggml-org/SmolVLM-256M-Instruct-GGUF/resolve/main/SmolVLM-256M-Instruct-Q8_0.gguf")!
        let vlmMmprojURL = URL(string: "https://huggingface.co/ggml-org/SmolVLM-256M-Instruct-GGUF/resolve/main/mmproj-SmolVLM-256M-Instruct-f16.gguf")!
        
        RunAnywhere.registerMultiFileModel(
            id: vlmModelId,
            name: "SmolVLM 256M Instruct (Q8)",
            files: [
                ModelFileDescriptor(url: vlmModelURL, filename: "SmolVLM-256M-Instruct-Q8_0.gguf"),
                ModelFileDescriptor(url: vlmMmprojURL, filename: "mmproj-SmolVLM-256M-Instruct-f16.gguf"),
            ],
            framework: .llamaCpp,
            modality: .multimodal,
            memoryRequirement: 365_000_000
        )
        
        // Register Diffusion model - Apple Stable Diffusion 1.5 CoreML (palettized, split_einsum_v2 for ANE)
        if let sd15URL = URL(string: "https://huggingface.co/apple/coreml-stable-diffusion-v1-5-palettized/resolve/main/coreml-stable-diffusion-v1-5-palettized_split_einsum_v2_compiled.zip") {
            RunAnywhere.registerModel(
                id: diffusionModelId,
                name: "Stable Diffusion 1.5 (CoreML)",
                url: sd15URL,
                framework: .coreml,
                modality: .imageGeneration,
                artifactType: .archive(.zip, structure: .nestedDirectory),
                memoryRequirement: 1_600_000_000
            )
        }
        
        print("✅ Models registered: LLM, STT, TTS, VLM, Diffusion")
    }
    
    // MARK: - State Refresh
    func refreshLoadedStates() async {
        isLLMLoaded = await RunAnywhere.isModelLoaded
        isSTTLoaded = await RunAnywhere.isSTTModelLoaded
        isTTSLoaded = await RunAnywhere.isTTSVoiceLoaded
        isVLMLoaded = await RunAnywhere.isVLMModelLoaded
        isDiffusionLoaded = await RunAnywhere.isDiffusionModelLoaded
    }
    
    // MARK: - LLM Operations
    /// Download and load LLM model
    func downloadAndLoadLLM() async {
        guard !isLLMDownloading && !isLLMLoading else { return }
        
        // Try to load first if already downloaded
        isLLMLoading = true
        do {
            try await RunAnywhere.loadModel(Self.llmModelId)
            isLLMLoaded = true
            isLLMLoading = false
            print("✅ LLM model loaded from cache")
            return
        } catch {
            print("LLM load attempt failed (will download): \(error)")
            isLLMLoading = false
        }
        
        // If loading failed, download the model
        isLLMDownloading = true
        llmDownloadProgress = 0.0
        
        do {
            let progressStream = try await RunAnywhere.downloadModel(Self.llmModelId)
            for await progress in progressStream {
                llmDownloadProgress = progress.overallProgress
                if progress.stage == .completed {
                    break
                }
            }
        } catch {
            print("LLM download error: \(error)")
            isLLMDownloading = false
            return
        }
        
        isLLMDownloading = false
        
        // Load the model after download
        isLLMLoading = true
        
        do {
            try await RunAnywhere.loadModel(Self.llmModelId)
            isLLMLoaded = true
        } catch {
            print("LLM load error: \(error)")
        }
        
        isLLMLoading = false
    }
    
    // MARK: - STT Operations
    /// Download and load STT model
    func downloadAndLoadSTT() async {
        guard !isSTTDownloading && !isSTTLoading else { return }
        
        // Try to load first if already downloaded
        isSTTLoading = true
        do {
            try await RunAnywhere.loadSTTModel(Self.sttModelId)
            isSTTLoaded = true
            isSTTLoading = false
            print("✅ STT model loaded from cache")
            return
        } catch {
            print("STT load attempt failed (will download): \(error)")
            isSTTLoading = false
        }
        
        // If loading failed, download the model
        isSTTDownloading = true
        sttDownloadProgress = 0.0
        
        do {
            let progressStream = try await RunAnywhere.downloadModel(Self.sttModelId)
            for await progress in progressStream {
                sttDownloadProgress = progress.overallProgress
                if progress.stage == .completed {
                    break
                }
            }
        } catch {
            print("STT download error: \(error)")
            isSTTDownloading = false
            return
        }
        
        isSTTDownloading = false
        
        // Load the model after download
        isSTTLoading = true
        
        do {
            try await RunAnywhere.loadSTTModel(Self.sttModelId)
            isSTTLoaded = true
        } catch {
            print("STT load error: \(error)")
        }
        
        isSTTLoading = false
    }
    
    // MARK: - TTS Operations
    /// Download and load TTS voice
    func downloadAndLoadTTS() async {
        guard !isTTSDownloading && !isTTSLoading else { return }
        
        // Try to load first if already downloaded
        isTTSLoading = true
        do {
            try await RunAnywhere.loadTTSVoice(Self.ttsModelId)
            isTTSLoaded = true
            isTTSLoading = false
            print("✅ TTS voice loaded from cache")
            return
        } catch {
            print("TTS load attempt failed (will download): \(error)")
            isTTSLoading = false
        }
        
        // If loading failed, download the model
        isTTSDownloading = true
        ttsDownloadProgress = 0.0
        
        do {
            let progressStream = try await RunAnywhere.downloadModel(Self.ttsModelId)
            for await progress in progressStream {
                ttsDownloadProgress = progress.overallProgress
                if progress.stage == .completed {
                    break
                }
            }
        } catch {
            print("TTS download error: \(error)")
            isTTSDownloading = false
            return
        }
        
        isTTSDownloading = false
        
        // Load the voice after download
        isTTSLoading = true
        
        do {
            try await RunAnywhere.loadTTSVoice(Self.ttsModelId)
            isTTSLoaded = true
        } catch {
            print("TTS load error: \(error)")
        }
        
        isTTSLoading = false
    }
    
    // MARK: - VLM Operations
    /// Download and load VLM model (SmolVLM 256M - multimodal)
    func downloadAndLoadVLM() async {
        guard !isVLMDownloading && !isVLMLoading else { return }
        
        // Try to load first if already downloaded
        isVLMLoading = true
        do {
            let models = try await RunAnywhere.availableModels()
            if let vlmModel = models.first(where: { $0.id == Self.vlmModelId && $0.isDownloaded }) {
                try await RunAnywhere.loadVLMModel(vlmModel)
                isVLMLoaded = true
                isVLMLoading = false
                print("✅ VLM model loaded from cache")
                return
            }
        } catch {
            print("VLM load attempt failed (will download): \(error)")
        }
        isVLMLoading = false
        
        // Download the model
        isVLMDownloading = true
        vlmDownloadProgress = 0.0
        
        do {
            let progressStream = try await RunAnywhere.downloadModel(Self.vlmModelId)
            for await progress in progressStream {
                vlmDownloadProgress = progress.overallProgress
                if progress.stage == .completed {
                    break
                }
            }
        } catch {
            print("VLM download error: \(error)")
            isVLMDownloading = false
            return
        }
        
        isVLMDownloading = false
        
        // Load the model after download
        isVLMLoading = true
        
        do {
            let models = try await RunAnywhere.availableModels()
            if let vlmModel = models.first(where: { $0.id == Self.vlmModelId }) {
                try await RunAnywhere.loadVLMModel(vlmModel)
                isVLMLoaded = true
            } else {
                print("VLM model not found in registry after download")
            }
        } catch {
            print("VLM load error: \(error)")
        }
        
        isVLMLoading = false
    }
    
    // MARK: - Diffusion Operations
    /// Download and load Diffusion model (Stable Diffusion 1.5 CoreML)
    /// Note: First-time CoreML compilation can take 5-15 minutes
    func downloadAndLoadDiffusion() async {
        guard !isDiffusionDownloading && !isDiffusionLoading else { return }
        
        // Try to load first if already downloaded
        isDiffusionLoading = true
        diffusionStatusMessage = "Checking for cached model..."
        do {
            let models = try await RunAnywhere.availableModels()
            if let model = models.first(where: { $0.id == Self.diffusionModelId && $0.isDownloaded }),
               let path = model.localPath {
                diffusionStatusMessage = "Loading CoreML pipeline (first time may take 5-15 min)..."
                let config = DiffusionConfiguration(
                    modelVariant: .sd15,
                    enableSafetyChecker: true,
                    reduceMemory: true
                )
                try await RunAnywhere.loadDiffusionModel(
                    modelPath: path.path,
                    modelId: model.id,
                    modelName: model.name,
                    configuration: config
                )
                isDiffusionLoaded = true
                isDiffusionLoading = false
                diffusionStatusMessage = "Model loaded"
                print("✅ Diffusion model loaded from cache")
                return
            }
        } catch {
            print("Diffusion load attempt failed (will download): \(error)")
        }
        isDiffusionLoading = false
        
        // Download the model
        isDiffusionDownloading = true
        diffusionDownloadProgress = 0.0
        diffusionStatusMessage = "Downloading (~1.6 GB)..."
        
        do {
            let progressStream = try await RunAnywhere.downloadModel(Self.diffusionModelId)
            for await progress in progressStream {
                diffusionDownloadProgress = progress.overallProgress
                if progress.stage == .completed {
                    break
                }
            }
        } catch {
            print("Diffusion download error: \(error)")
            diffusionStatusMessage = "Download failed"
            isDiffusionDownloading = false
            return
        }
        
        isDiffusionDownloading = false
        
        // Load the model after download
        isDiffusionLoading = true
        diffusionStatusMessage = "Loading CoreML pipeline (first time may take 5-15 min)..."
        
        do {
            let models = try await RunAnywhere.availableModels()
            if let model = models.first(where: { $0.id == Self.diffusionModelId }),
               let path = model.localPath {
                let config = DiffusionConfiguration(
                    modelVariant: .sd15,
                    enableSafetyChecker: true,
                    reduceMemory: true
                )
                try await RunAnywhere.loadDiffusionModel(
                    modelPath: path.path,
                    modelId: model.id,
                    modelName: model.name,
                    configuration: config
                )
                isDiffusionLoaded = true
                diffusionStatusMessage = "Model loaded"
            } else {
                print("Diffusion model not found in registry after download")
                diffusionStatusMessage = "Model not found after download"
            }
        } catch {
            print("Diffusion load error: \(error)")
            diffusionStatusMessage = "Load failed: \(error.localizedDescription)"
        }
        
        isDiffusionLoading = false
    }
    
    // MARK: - Batch Operations
    /// Download and load all models for voice agent
    /// Note: Downloads run sequentially to avoid SDK concurrency issues
    func downloadAndLoadAllModels() async {
        // Run sequentially to avoid race conditions in SDK's download service
        await downloadAndLoadLLM()
        await downloadAndLoadSTT()
        await downloadAndLoadTTS()
    }
    
    /// Unload all models
    func unloadAllModels() async {
        do {
            try await RunAnywhere.unloadModel()
        } catch {
            print("LLM unload error: \(error)")
        }
        
        do {
            try await RunAnywhere.unloadSTTModel()
        } catch {
            print("STT unload error: \(error)")
        }
        
        do {
            try await RunAnywhere.unloadTTSVoice()
        } catch {
            print("TTS unload error: \(error)")
        }
        
        await refreshLoadedStates()
    }
}
